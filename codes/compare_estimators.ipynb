{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Wage Simulation -- Estimator Comparisons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set wd to Users/boyuchen/Documents/UBC/RA/minimum_wage\n",
    "os.chdir('/Users/boyuchen/Documents/UBC/RA/minimum_wage')\n",
    "\n",
    "from codes.simulation_functions import *\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**: demonstrate whether the three methods worked as expected.\n",
    "\n",
    "**TL;DR:** \n",
    "\n",
    "1. Scenarios of choice: two truncated scenarios with no bunching and spillovers; then one with bunching and one with spillover.\n",
    "2. Cengiz et al(2019) did a good job in not producing spurious spillover effect.\n",
    "3. Percentile method gives the spurious \"spillover effect\" in S1 to S3.\n",
    "4. My hazard estimates results have significant effect on bins above the minimum wage bin, which is not what I expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_wages = generate_log_wages(1_000_000)  \n",
    "# run df_grouped for four proability scenarios:\n",
    "# real_m_pre, real_m_post, P_o, P_b, P_s\n",
    "scenarios = {\n",
    "    'S1': (12, 14, 0.5, 0, 0),\n",
    "    'S2': (12, 14, 0.5, 0.1, 0),\n",
    "    'S3': (12, 14, 0.5, 0, 0.3),\n",
    "    'S4': (14, 16, 0.5, 0, 0),\n",
    "    'S5': (14, 16, 0.5, 0.1, 0),\n",
    "    'S6': (14, 16, 0.5, 0, 0.3)\n",
    "}\n",
    "\n",
    "# scenarios = {\n",
    "#     'S1': (10.05, 12.05, 0.3, 0.5, 0),\n",
    "#     'S2': (10.05, 12.05, 0.3, 0, 0.5),\n",
    "#     'S3': (13.05, 15.05, 0.3, 0.5, 0),\n",
    "#     'S4': (13.05, 15.05, 0.3, 0, 0.5),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive percentile OLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(log_wages, scenario):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "    df_pre = impose_minimum_wage(log_wages, real_m_pre, P_o, P_b, P_s)\n",
    "    df_post = impose_minimum_wage(log_wages, real_m_post, P_o, P_b, P_s)\n",
    "\n",
    "\n",
    "    def to_real_scale(log_wages):\n",
    "        return np.exp(log_wages)\n",
    "\n",
    "    df_pre['real_wages'] = to_real_scale(df_pre['adjusted_log_wages'])\n",
    "    df_post['real_wages'] = to_real_scale(df_post['adjusted_log_wages'])\n",
    "\n",
    "    percentiles = np.arange(0, 85, 5)\n",
    "\n",
    "    pre_percentiles = np.percentile(df_pre['real_wages'].dropna(), percentiles)\n",
    "    post_percentiles = np.percentile(df_post['real_wages'].dropna(), percentiles)\n",
    "    \n",
    "    # Calculate 95th percentile\n",
    "    p95_pre = np.percentile(df_pre['real_wages'].dropna(), 95)\n",
    "    p95_post = np.percentile(df_post['real_wages'].dropna(), 95)\n",
    "    x_limit = max(p95_pre, p95_post)\n",
    "\n",
    "    return pre_percentiles, post_percentiles, df_pre['real_wages'].dropna(), df_post['real_wages'].dropna(), x_limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the relative changes in precentiles as the minimum wage changes. As expected, all percentiles \"inflated\" as some more people got unemployed. We got bad results as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the figure and axes for subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8), sharey=True)\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate and plot results for each scenario\n",
    "for i, (scenario_name, params) in enumerate(scenarios.items()):\n",
    "    pre_percentiles, post_percentiles, _, _, x_limit = calculate_percentiles(log_wages, params)\n",
    "    relative_changes = (post_percentiles - pre_percentiles) / pre_percentiles * 100\n",
    "\n",
    "    ax = axes[i]\n",
    "    percentiles = np.arange(5, 85, 5)\n",
    "\n",
    "\n",
    "    ax.bar(percentiles, relative_changes[1:], width=4.5, alpha=0.7, color='green')\n",
    "    ax.set_xlabel('Percentiles')\n",
    "    # ax.set_ylabel('Relative Change (%)')\n",
    "    ax.set_title(f'{scenario_name}: min wage {params[0]} to {params[1]}, $P_o$={params[2]}, $P_b$={params[3]}, $P_s$={params[4]}')\n",
    "    ax.set_xticks(percentiles)\n",
    "    ax.set_xlim([-5, 85])\n",
    "    ax.set_ylim([min(relative_changes[1:]) - 5, max(relative_changes[1:]) + 5])\n",
    "\n",
    "# Set the main title\n",
    "plt.suptitle('Relative Changes in Wage Percentiles Across Scenarios', fontsize=16)\n",
    "fig.text(0.04, 0.5, 'Relative Change (%)', va='center', rotation='vertical')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cengiz et al.(2019) Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I replicated and simplified Cengiz method. In S1 an S2, there are strong employment impact on bins right below the minimum wage bin. In S3, the method captures only bunching. In S4, the model captures the spillover effect. Overall the method produces good results that reflect the underlying model.  Bin size is 0.25.\n",
    "\n",
    "*Notes: Since we use a two-period model, the old minimum wage  yields a visible trend near the level, which is not visible in their results since it has many period and states and the effect of \"old\" minimum wage is washed out.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Cengiz method for employment changes and wage effects\n",
    "def cengiz_method(log_wages, scenario, no_minimum_wage=False):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "    m_pre = np.log(real_m_pre)\n",
    "    m_post = np.log(real_m_post)\n",
    "\n",
    "    if no_minimum_wage:\n",
    "        real_m_pre = 0\n",
    "        \n",
    "    # pass no_minimum_wage argument to impose_minimum_wage function\n",
    "    df_pre = impose_minimum_wage(log_wages, real_m_pre, P_o, P_b, P_s)\n",
    "    df_post = impose_minimum_wage(log_wages, real_m_post, P_o, P_b, P_s)\n",
    "\n",
    "    df_pre['real_wages'] = np.exp(df_pre['adjusted_log_wages'])\n",
    "    df_post['real_wages'] = np.exp(df_post['adjusted_log_wages'])\n",
    "    \n",
    "    # Define the 75 percentile of the pre-wage distribution (Wbar)\n",
    "    Wbar = df_pre['real_wages'].quantile(0.75)\n",
    "\n",
    "    bin_width = 0.1\n",
    "\n",
    "    # Assign bins for pre and post data, excluding unemployed\n",
    "    bins_pre = np.arange(0, Wbar + bin_width, bin_width)\n",
    "    bins_post = np.arange(0, Wbar + bin_width, bin_width)\n",
    "\n",
    "    df_pre['wage_bin'] = pd.cut(df_pre['real_wages'], bins=bins_pre, right=False)\n",
    "    df_post['wage_bin'] = pd.cut(df_post['real_wages'], bins=bins_post, right=False)\n",
    "\n",
    "    # Calculate the pre-period average wage of workers below the initial minimum wage\n",
    "    affected_pre = df_pre[df_pre['original_log_wages'] < m_pre]\n",
    "    # affected_post = df_post[df_post['original_log_wages'] < m_pre]\n",
    "\n",
    "    average_affected_wage_pre = affected_pre['real_wages'].mean()\n",
    "    affected_wage_bill_pre = affected_pre['real_wages'].sum()\n",
    "    total_wage_bill_pre = df_pre['real_wages'].sum()\n",
    "    total_wage_bill_post = df_post['real_wages'].sum()\n",
    "    Δtotal_wage_bill = total_wage_bill_post - total_wage_bill_pre\n",
    "    \n",
    "    # Count the number of workers in each wage bin, excluding unemployed\n",
    "    pre_counts = df_pre['wage_bin'].value_counts().sort_index()\n",
    "    post_counts = df_post['wage_bin'].value_counts().sort_index()\n",
    "\n",
    "    if no_minimum_wage:\n",
    "        return {\n",
    "            'pre_counts': pre_counts,\n",
    "            'post_counts': post_counts,\n",
    "            'bins_pre': pre_counts.index.tolist()\n",
    "        }\n",
    "    else:\n",
    "\n",
    "        # count the total number of affected worker in the pre-period using affected_pre\n",
    "        total_affected_pre = affected_pre.shape[0]\n",
    "\n",
    "        pre_counts = pre_counts.reindex_like(post_counts).fillna(0)\n",
    "        post_counts = post_counts.reindex_like(pre_counts).fillna(0)\n",
    "\n",
    "        # Create a boolean mask for intervals where the left edge is greater than or equal to m_post\n",
    "        mask_post = np.array([interval.left >= np.exp(m_post).round(3) for interval in post_counts.index])\n",
    "        mask_pre = np.array([interval.left >= np.exp(m_post).round(3) for interval in pre_counts.index])\n",
    "\n",
    "        # Calculate excess and missing jobs, only considering bins below Wbar\n",
    "        Δa = post_counts[mask_post].sum() - pre_counts[mask_pre].sum()\n",
    "        Δb = post_counts[~mask_post].sum() - pre_counts[~mask_pre].sum()\n",
    "        Δemp = Δa + Δb\n",
    "\n",
    "        total_employed_pre = pre_counts.sum()\n",
    "\n",
    "        Δm = real_m_post - real_m_pre\n",
    "        employment_elasticity = (Δemp / total_employed_pre) / (Δm / real_m_pre)\n",
    "\n",
    "        # change in total employment over the \"affected workers\" in the pre-period\n",
    "        affected_employment_change = Δemp / total_affected_pre\n",
    "\n",
    "        # change in wage bills of affected workers is the pre-period affected wage bill + the total change in wage bill of all workers\n",
    "        affected_new_wage_bill = affected_wage_bill_pre + Δtotal_wage_bill\n",
    "        affected_employment_post = total_affected_pre + Δemp\n",
    "\n",
    "        # calculate percentage change in the average wage of affected workers\n",
    "        new_average_wage_affected = affected_new_wage_bill / affected_employment_post\n",
    "\n",
    "        # calculate percentage change in the average wage of affected workers\n",
    "        percentage_change_average_wage_affected = (new_average_wage_affected - average_affected_wage_pre) / average_affected_wage_pre\n",
    "\n",
    "        # calculate own_elasticity \n",
    "        own_wage_elasticity = affected_employment_change / percentage_change_average_wage_affected\n",
    "\n",
    "        return {\n",
    "            'employment_elasticity': employment_elasticity,\n",
    "            'own_wage_elasticity': own_wage_elasticity,\n",
    "            'percentage_change_affected_employment': affected_employment_change,\n",
    "            'percentage_change_average_wage_affected': percentage_change_average_wage_affected,\n",
    "            'Δa': Δa,\n",
    "            'Δb': Δb,\n",
    "            'bins_pre': pre_counts.index.tolist(),\n",
    "            'pre_counts': pre_counts,\n",
    "            'post_counts': post_counts\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_changes_in_employment(result, scenario_name, scenario, no_minimum_wage=False):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "    pre_counts = result['pre_counts']\n",
    "    post_counts = result['post_counts']\n",
    "    bins_pre = result['bins_pre']\n",
    "\n",
    "    if no_minimum_wage==False:\n",
    "\n",
    "        Δa = result['Δa']\n",
    "        Δb = result['Δb']\n",
    "        percentage_change_affected_employment = result['percentage_change_affected_employment']\n",
    "        percentage_change_average_wage_affected = result['percentage_change_average_wage_affected']\n",
    "\n",
    "    # Calculate the changes in employment\n",
    "    employment_changes = post_counts - pre_counts\n",
    "\n",
    "    # Calculate the total pre-employment\n",
    "    total_pre_employment = pre_counts.sum()\n",
    "\n",
    "    # Calculate relative changes to pre-total employment\n",
    "    relative_employment_changes = employment_changes / total_pre_employment * 100\n",
    "\n",
    "    # Extract intervals for display\n",
    "    intervals = [interval.left for interval in bins_pre]\n",
    "\n",
    "    # Calculate bins relative to real_m_post\n",
    "    bins_relative_to_post = [round((interval - real_m_post) / (bins_pre[1].left - bins_pre[0].left)) for interval in intervals]\n",
    "\n",
    "    # Find the positions for m_pre and m_post\n",
    "    m_pre_pos = next(i for i, interval in enumerate(intervals) if interval >= real_m_pre)\n",
    "    m_post_pos = next(i for i, interval in enumerate(intervals) if interval >= real_m_post)\n",
    "\n",
    "    # Select 10 bins below real_m_pre and 30 bins above real_m_post\n",
    "    start_idx = max(0, m_pre_pos - 4)\n",
    "    end_idx = min(len(intervals), m_post_pos + 20)\n",
    "\n",
    "    bins_relative_to_post = bins_relative_to_post[start_idx:end_idx]\n",
    "    relative_employment_changes = relative_employment_changes[start_idx:end_idx]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    index = np.arange(len(bins_relative_to_post))\n",
    "\n",
    "    ax.bar(index, relative_employment_changes, bar_width, alpha=0.7, color='green', label='Change in Employment')\n",
    "\n",
    "    ax.set_xlabel('Bins relative to new MW')\n",
    "    # ax.set_ylabel('% Change in Employment Relative to Pre-Total Employment')\n",
    "    ax.set_title(f'{scenario_name}: minimum wage from {real_m_pre} to {real_m_post}, $P_o$={P_o}, $P_b$={P_b}, $P_s$={P_s}')\n",
    "    ax.set_xticks(index[::2])\n",
    "    ax.set_xticklabels(bins_relative_to_post[::2], rotation=90)\n",
    "\n",
    "    # Add grey line at y=0\n",
    "    ax.axhline(y=0, color='grey', linestyle='-', linewidth=1)\n",
    "\n",
    "    # Adjust positions for m_pre and m_post lines to match the selected bins\n",
    "    m_pre_pos_adj = m_pre_pos - start_idx\n",
    "    m_post_pos_adj = m_post_pos - start_idx\n",
    "\n",
    "    ax.set_ylim(-5, 5)\n",
    "\n",
    "    if no_minimum_wage==False:\n",
    "        # Add m_pre and m_post lines\n",
    "        ax.axvline(m_pre_pos_adj - 0.5, color='blue', linestyle='--', linewidth=2, label='original minimum wage')\n",
    "        ax.axvline(m_post_pos_adj - 0.5, color='red', linestyle='--', linewidth=2, label='new minimum wage')\n",
    "\n",
    "        # Add parameters to the lower left\n",
    "        ax.text(0.62, 0.1,\n",
    "                f'Δa = {Δa:.3f}\\nΔb = {Δb:.3f}\\n%Δ affected emp. = {percentage_change_affected_employment:.3f}\\n%Δ affected wage = {percentage_change_average_wage_affected:.3f}',\n",
    "                bbox=dict(facecolor='white', alpha=0.5), transform=ax.transAxes)\n",
    "\n",
    "    # ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "# Generate and plot results for each scenario\n",
    "for ax, (scenario_name, scenario) in zip(axs.flatten(), scenarios.items()):\n",
    "    result = cengiz_method(log_wages, scenario, no_minimum_wage=True)\n",
    "    plot_changes_in_employment(result, scenario_name, scenario, no_minimum_wage=True)\n",
    "\n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'Percentage Change in Employment Relative to Pre-Total Employment (%)', va='center', rotation='vertical')\n",
    "plt.suptitle('Effect of the Minimum Wage on the Wage Distribution', fontsize=16)\n",
    "# only show legend for panel 3\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cengiz_data(log_wages, scenario):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "\n",
    "    total_population = log_wages.shape[0]\n",
    "\n",
    "    # Assume impose_minimum_wage is defined and applies the minimum wage policy to the data\n",
    "    df_pre = impose_minimum_wage(log_wages, real_m_pre, P_o, P_b, P_s)\n",
    "    df_post = impose_minimum_wage(log_wages, real_m_post, P_o, P_b, P_s)\n",
    "\n",
    "    df_pre['real_wages'] = np.exp(df_pre['adjusted_log_wages'])\n",
    "    df_post['real_wages'] = np.exp(df_post['adjusted_log_wages'])\n",
    "\n",
    "    # Simplify bins\n",
    "    bins = [0] + list(np.arange(3, 4, 0.5)) + list(np.arange(4, 20, 0.1)) + [20, np.inf]\n",
    "\n",
    "    def create_min_dummies(df, real_m):\n",
    "        # Create bins and labels\n",
    "        df['wagcat'] = pd.cut(df['real_wages'], bins=bins, labels=False, duplicates='drop')\n",
    "        # Change wagcat to be integer or NaN\n",
    "        df['wagcat'] = df['wagcat'].astype('Int64')\n",
    "\n",
    "        # Find the bin number containing the minimum wage\n",
    "        m_bin = pd.cut([real_m], bins=bins, labels=False, duplicates='drop')[0]\n",
    "\n",
    "        # Aggregate data by bin\n",
    "        df_grouped = df.groupby('wagcat').size().reset_index(name='fweight')\n",
    "        df_grouped['normalized_emp'] = df_grouped['fweight'] / total_population\n",
    "        max_bin = df_grouped['wagcat'].max()\n",
    "        all_bins = pd.DataFrame({'wagcat': range(max_bin + 1)})\n",
    "        df_grouped = pd.merge(all_bins, df_grouped, on='wagcat', how='left').fillna(0)\n",
    "\n",
    "        # Create min and variables that the relative position to the minimum wage\n",
    "        df_grouped['rel_pos'] = df_grouped['wagcat'] - m_bin\n",
    "\n",
    "        # Create bin* dummies\n",
    "        bin_dummies = pd.get_dummies(df_grouped['wagcat'], prefix='bin')\n",
    "\n",
    "        for i in range(20):\n",
    "            # create min* dummies\n",
    "            df_grouped[f'min{i+1}b'] = (df_grouped['rel_pos'] == -(i+1)).astype(int)\n",
    "\n",
    "        for i in range(20):\n",
    "            # create min* dummies\n",
    "            df_grouped[f'min{i+1}a'] = (df_grouped['rel_pos'] == i+1).astype(int)\n",
    "\n",
    "        df_grouped['min0'] = df_grouped['rel_pos'] == 0\n",
    "\n",
    "        df_grouped = pd.concat([df_grouped, bin_dummies], axis=1)\n",
    "        \n",
    "        return df_grouped\n",
    "    \n",
    "    # Create min dummies and group by bin-period for both pre and post data\n",
    "    df_grouped_pre = create_min_dummies(df_pre, real_m_pre)\n",
    "    df_grouped_pre['period'] = 'pre'\n",
    "\n",
    "    df_grouped_post = create_min_dummies(df_post, real_m_post)\n",
    "    df_grouped_post['period'] = 'post'\n",
    "\n",
    "    # Combine pre and post data\n",
    "    df_grouped = pd.concat([df_grouped_pre, df_grouped_post])\n",
    "    df_grouped = df_grouped.astype({col: 'int64' if df_grouped[col].dtype == 'bool' else df_grouped[col].dtype for col in df_grouped.columns})\n",
    "    df_grouped = sm.add_constant(df_grouped)\n",
    "\n",
    "    # drop lowest 5 and top 5 bins\n",
    "    df_grouped = df_grouped[(df_grouped['wagcat'] >= 5) & (df_grouped['wagcat'] <= 160)]\n",
    "\n",
    "    # Sanity check 1: sum of min* variables must be 0 or 1\n",
    "    min_vars = [col for col in df_grouped.columns if col.startswith('min')]\n",
    "    df_grouped['min_sum'] = df_grouped[min_vars].sum(axis=1)\n",
    "    assert df_grouped['min_sum'].isin([0, 1]).all(), \"Sanity check failed: sum of min* variables is not 0 or 1 for some rows.\"\n",
    "\n",
    "    # Sanity check 2: sum of bin* variables must be 1\n",
    "    bin_vars = [col for col in df_grouped.columns if col.startswith('bin')]\n",
    "    df_grouped['bin_sum'] = df_grouped[bin_vars].sum(axis=1)\n",
    "    assert (df_grouped['bin_sum'] == 1).all(), \"Sanity check failed: sum of bin* variables is not 1 for some rows.\"\n",
    "\n",
    "    # drop min_sum and bin_sum\n",
    "    df_grouped = df_grouped.drop(columns=['min_sum', 'bin_sum'])\n",
    "\n",
    "\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cengiz_ols(log_wages, scenario):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "    data = prepare_cengiz_data(log_wages, scenario)\n",
    "    \n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "    data = prepare_cengiz_data(log_wages, scenario)\n",
    "    # Prepare the independent variables (min* and bin* dummies)\n",
    "    X = data.filter(regex='^(min\\\\d{1,2}|bin)').copy()\n",
    "    y = data['normalized_emp']\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    # Generate and plot results for each scenario\n",
    "    model.params.filter(regex='^(min\\\\d{1,2}|min0)')\n",
    "\n",
    "    paras = model.params.filter(regex='^(min\\\\d{1,2}|min0)')\n",
    "    coef_df = pd.DataFrame(paras).reset_index()\n",
    "    # rename column \"0\" as \"coefficient\"\n",
    "    coef_df = coef_df.rename(columns={0: 'coefficient'})\n",
    "\n",
    "    # for min[n]b, assign rel_pos = -n; for min0, assign rel_pos = 0; for min[n]a, assign rel_pos = n\n",
    "    coef_df['rel_pos'] = coef_df['index'].str.extract('(\\d{1,2})').astype(int)\n",
    "    coef_df['rel_pos'] = np.where(coef_df['index'].str.contains('a'), coef_df['rel_pos'], -coef_df['rel_pos'])\n",
    "    # sort coef_df by rel_pos\n",
    "    coef_df = coef_df.sort_values('rel_pos')\n",
    "\n",
    "    # plot bar plot\n",
    "    ax.figsize=(12, 6)\n",
    "    ax.bar(coef_df['rel_pos'], coef_df['coefficient'], color='green', alpha=0.7)\n",
    "    ax.set_xlabel('Bins relative to new MW')\n",
    "    ax.set_ylabel('Coefficient')\n",
    "    ax.set_ylim(-0.01, 0.03)\n",
    "    ax.set_title(f'{scenario_name}: minimum wage from {real_m_pre} to {real_m_post}, $P_o$={P_o}, $P_b$={P_b}, $P_s$={P_s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "# Generate and plot results for each scenario\n",
    "for ax, (scenario_name, scenario) in zip(axs.flatten(), scenarios.items()):\n",
    "    plot_cengiz_ols(log_wages, scenario)\n",
    "    \n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'Percentage Change in Employment Relative to Pre-Total Employment (%)', va='center', rotation='vertical')\n",
    "plt.suptitle('Effect of the Minimum Wage on the Wage Distribution', fontsize=16)\n",
    "# only show legend for panel 3\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cengiz_counterfactual(log_wages, scenario):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenario\n",
    "    data = prepare_cengiz_data(log_wages, scenario)\n",
    "    \n",
    "    # Prepare the independent variables (min* and bin* dummies)\n",
    "    X = data.filter(regex='^(min\\\\d{1,2}|bin)').copy()\n",
    "    y = data['normalized_emp']\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    # Generate and plot results for each scenario\n",
    "    model.params.filter(regex='^(min\\\\d{1,2}|min0)')\n",
    "\n",
    "    # Create a copy of the original data\n",
    "    df_counterfactual = data.copy()\n",
    "    # Identify columns starting with 'min'\n",
    "    min_columns = [col for col in df_counterfactual.columns if col.startswith('min')]\n",
    "    # Set these columns to 0\n",
    "    df_counterfactual[min_columns] = 0 \n",
    "    # predict counterfactual employment\n",
    "    df_counterfactual['pred_emp'] = model.predict(df_counterfactual[X.columns])\n",
    "    # calculate the difference between actual and predicted employment using post period data\n",
    "    # tot_emp_pre = df_counterfactual[df_counterfactual['period'] == 'pre']['normalized_emp'].sum()\n",
    "    df_counterfactual_post = df_counterfactual[df_counterfactual['period'] == 'post']\n",
    "    df_counterfactual_post['emp_diff'] = df_counterfactual_post['normalized_emp'] - df_counterfactual_post['pred_emp']\n",
    "\n",
    "    # sort from lowest bin\n",
    "    df_counterfactual_post = df_counterfactual_post.sort_values('rel_pos')\n",
    "\n",
    "    # running sum of emp change \n",
    "    df_counterfactual_post['emp_diff_sum'] = df_counterfactual_post['emp_diff'].cumsum()\n",
    "\n",
    "    # find rows with largest emp_diff\n",
    "    df_counterfactual_post.nlargest(10, 'emp_diff')\n",
    "\n",
    "    ax.bar(df_counterfactual_post['rel_pos'], df_counterfactual_post['emp_diff'], width=1, color='blue', alpha=0.7, label='Change in Employment')\n",
    "\n",
    "    ax.set_xlabel('Bins relative to new MW')\n",
    "    # ax.set_ylabel('% Change in Employment Relative to Pre-Total Employment')\n",
    "    ax.set_title(f'{scenario_name}: minimum wage from {real_m_pre} to {real_m_post}, $P_o$={P_o}, $P_b$={P_b}, $P_s$={P_s}')\n",
    "    ax.set_xlim(-20, 20)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "# Generate and plot results for each scenario\n",
    "for ax, (scenario_name, scenario) in zip(axs.flatten(), scenarios.items()):\n",
    "    plot_cengiz_counterfactual(log_wages, scenario)\n",
    "\n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'Percentage Change in Employment Relative to Pre-Total Employment (%)', va='center', rotation='vertical')\n",
    "plt.suptitle('Effect of the Minimum Wage on the Wage Distribution', fontsize=16)\n",
    "# only show legend for panel 3\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard Estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first prepare the data and variables following the definition that Thomas gave me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hazard_var(log_wages, real_m_pre, real_m_post, P_o=0.2, P_b=0, P_s=0.5, append_unaffected_wages = False):\n",
    "\n",
    "    # Assuming impose_minimum_wage function is defined elsewhere\n",
    "    df_pre = impose_minimum_wage(log_wages, real_m_pre, P_o, P_b, P_s)\n",
    "    df_post = impose_minimum_wage(log_wages, real_m_post, P_o, P_b, P_s)\n",
    "    \n",
    "    df_pre['real_wages'] = np.exp(df_pre['adjusted_log_wages'])\n",
    "    df_post['real_wages'] = np.exp(df_post['adjusted_log_wages'])\n",
    "\n",
    "    # drop the unemployed\n",
    "    df_pre = df_pre.dropna(subset=['real_wages'])\n",
    "    df_post = df_post.dropna(subset=['real_wages'])\n",
    "\n",
    "    # Simplify bins\n",
    "    bins = [0] + list(np.arange(3, 4, 0.5)) + list(np.arange(4, 20, 0.1)) + [20, np.inf]\n",
    "    min_vars = ['min', 'min21b', 'min1620b', 'min1115b','min610b', 'min35b', 'min12b', 'min12a', 'min35a', 'min610a', 'min1115a', 'min1620a']\n",
    "\n",
    "    def create_min_dummies(df, real_m):\n",
    "        # Create bins and labels\n",
    "        df['wagcat'] = pd.cut(df['real_wages'], bins=bins, labels=False, duplicates='drop')\n",
    "        # Change wagcat to be integer or NaN\n",
    "        df['wagcat'] = df['wagcat'].astype('Int64')\n",
    "\n",
    "        # Find the bin number containing the minimum wage\n",
    "        m_bin = pd.cut([real_m], bins=bins, labels=False, duplicates='drop')[0]\n",
    "\n",
    "        # Aggregate data by bin\n",
    "        df_grouped = df.groupby('wagcat').size().reset_index(name='fweight')\n",
    "        max_bin = df_grouped['wagcat'].max()\n",
    "        all_bins = pd.DataFrame({'wagcat': range(max_bin + 1)})\n",
    "        df_grouped = pd.merge(all_bins, df_grouped, on='wagcat', how='left').fillna(0)\n",
    "\n",
    "        # Create min and min* variables\n",
    "        df_grouped['min'] = np.where(df_grouped['wagcat'] == m_bin, 1, 0)\n",
    "        df_grouped['min21b'] = np.where((df_grouped['wagcat'] <= m_bin - 21), 1, 0)\n",
    "        df_grouped['min1620b'] = np.where((df_grouped['wagcat'] >= m_bin - 20) & (df_grouped['wagcat'] <= m_bin - 16), 1, 0)\n",
    "        df_grouped['min1115b'] = np.where((df_grouped['wagcat'] >= m_bin - 15) & (df_grouped['wagcat'] <= m_bin - 11), 1, 0)\n",
    "        df_grouped['min610b'] = np.where((df_grouped['wagcat'] >= m_bin - 10) & (df_grouped['wagcat'] <= m_bin - 6), 1, 0)\n",
    "        df_grouped['min35b'] = np.where((df_grouped['wagcat'] >= m_bin - 5) & (df_grouped['wagcat'] <= m_bin - 3), 1, 0)\n",
    "        df_grouped['min12b'] = np.where((df_grouped['wagcat'] >= m_bin - 2) & (df_grouped['wagcat'] <= m_bin - 1), 1, 0)\n",
    "        df_grouped['min12a'] = np.where((df_grouped['wagcat'] >= m_bin + 1) & (df_grouped['wagcat'] <= m_bin + 2), 1, 0)\n",
    "        df_grouped['min35a'] = np.where((df_grouped['wagcat'] >= m_bin + 3) & (df_grouped['wagcat'] <= m_bin + 5), 1, 0)\n",
    "        df_grouped['min610a'] = np.where((df_grouped['wagcat'] >= m_bin + 6) & (df_grouped['wagcat'] <= m_bin + 10), 1, 0)\n",
    "        df_grouped['min1115a'] = np.where((df_grouped['wagcat'] >= m_bin + 11) & (df_grouped['wagcat'] <= m_bin + 15), 1, 0)\n",
    "        df_grouped['min1620a'] = np.where((df_grouped['wagcat'] >= m_bin + 16) & (df_grouped['wagcat'] <= m_bin + 20), 1, 0)\n",
    "        df_grouped['min_bin'] = m_bin\n",
    "        df_grouped['rel_min_bin'] = df_grouped['wagcat'] - m_bin\n",
    "\n",
    "        # Create bin* dummies\n",
    "        bin_dummies = pd.get_dummies(df_grouped['wagcat'], prefix='bin')\n",
    "        df_grouped = pd.concat([df_grouped, bin_dummies], axis=1)\n",
    "        \n",
    "        return df_grouped\n",
    "\n",
    "    # Create min dummies and group by bin-period for both pre and post data\n",
    "    df_grouped_pre = create_min_dummies(df_pre, real_m_pre)\n",
    "    df_grouped_pre['period'] = 'pre'\n",
    "\n",
    "    df_grouped_post = create_min_dummies(df_post, real_m_post)\n",
    "    df_grouped_post['period'] = 'post'\n",
    "\n",
    "    # Combine pre and post data\n",
    "    df_grouped = pd.concat([df_grouped_pre, df_grouped_post])\n",
    "\n",
    "    if append_unaffected_wages:\n",
    "        df_raw = pd.DataFrame()\n",
    "        df_raw['real_wages'] = np.exp(log_wages)\n",
    "        \n",
    "        # Create bins and labels\n",
    "        df_raw['wagcat'] = pd.cut(df_raw['real_wages'], bins=bins, labels=False, duplicates='drop')\n",
    "        # Change wagcat to be integer or NaN\n",
    "        df_raw['wagcat'] = df_raw['wagcat'].astype('Int64')\n",
    "        \n",
    "        # Aggregate data by bin\n",
    "        df_grouped_raw = df_raw.groupby('wagcat').size().reset_index(name='fweight')\n",
    "        # define all var in min_vars to be 0\n",
    "        df_grouped_raw[min_vars] = 0\n",
    "        # Create bin* dummies\n",
    "        bin_dummies = pd.get_dummies(df_grouped_raw['wagcat'], prefix='bin')\n",
    "        df_grouped_raw = pd.concat([df_grouped_raw, bin_dummies], axis=1)\n",
    "        df_grouped_raw['period'] = 'counterfactual'\n",
    "        df_grouped = pd.concat([df_grouped, df_grouped_raw])\n",
    "\n",
    "    max_bin = df_grouped['wagcat'].max()\n",
    "\n",
    "\n",
    "    # Calculate remain: cumulative sum of fweight in descending order of bins\n",
    "    df_grouped = df_grouped.sort_values(by=['period', 'wagcat'], ascending=[True, False])\n",
    "    df_grouped['remain'] = df_grouped.groupby('period')['fweight'].cumsum()\n",
    "    df_grouped['hazard'] = df_grouped['fweight'] / df_grouped['remain']\n",
    "\n",
    "    # assert that by group, remain - remain.shift(1) == fweight\n",
    "    df_grouped = df_grouped.sort_values(by=['period', 'wagcat'])\n",
    "\n",
    "    # # group by period: df_grouped['remain_diff'] = df_grouped['remain'].diff()\n",
    "    df_grouped['remain_diff'] = df_grouped.groupby('period')['remain'].diff()\n",
    "    df_grouped['lag_fweight'] = df_grouped.groupby('period')['fweight'].shift(1)\n",
    "\n",
    "    # check if df_grouped[(df_grouped['lag_fweight'] + df_grouped['remain_diff'] == 0) | df_grouped['remain_diff'].isna()].shape[0] == df_grouped.shape[0]\n",
    "    assert (df_grouped[(df_grouped['lag_fweight'] + df_grouped['remain_diff'] == 0) | df_grouped['remain_diff'].isna()].shape[0] == df_grouped.shape[0]), \"Sanity check failed: remain - remain.shift(1) != fweight for some rows\"\n",
    "\n",
    "    # check fweight > 0 for all rows\n",
    "    assert (df_grouped['fweight'] > 0).all(), \"Sanity check failed: fweight <= 0 for some rows\"\n",
    "\n",
    "    # generate ln(-ln(1 - h(y))) as cloglog_hazard\n",
    "    df_grouped['cloglog_hazard'] = np.log(-np.log(1 - df_grouped['fweight'] / df_grouped['remain']))\n",
    "\n",
    "    # check cloglog_hazard is not NaN if wagcat is not 161\n",
    "    assert df_grouped[df_grouped['wagcat'] != max_bin]['cloglog_hazard'].notna().all(), \"Sanity check failed: cloglog_hazard is NaN for some rows where wagcat != 161\"\n",
    "    #drop wagcat == 161\n",
    "    df_grouped = df_grouped[df_grouped['wagcat'] != max_bin]\n",
    "\n",
    "    # Sanity check 1: sum of min* variables must be 0 or 1\n",
    "    df_grouped['min_sum'] = df_grouped[min_vars].sum(axis=1)\n",
    "    assert df_grouped['min_sum'].isin([0, 1]).all(), \"Sanity check failed: sum of min* variables is not 0 or 1 for some rows.\"\n",
    "\n",
    "    # Sanity check 2: sum of bin* variables must be 1\n",
    "    bin_vars = [col for col in df_grouped.columns if col.startswith('bin')]\n",
    "    df_grouped['bin_sum'] = df_grouped[bin_vars].sum(axis=1)\n",
    "    assert (df_grouped['bin_sum'] == 1).all(), \"Sanity check failed: sum of bin* variables is not 1 for some rows.\"\n",
    "\n",
    "    # drop bin_sum and min_sum columns\n",
    "    df_grouped = df_grouped.drop(columns=['bin_sum', 'lag_fweight', 'remain_diff'])\n",
    "    \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data for four scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended = pd.DataFrame()\n",
    "\n",
    "# loop through the scenarios\n",
    "for scenario, params in scenarios.items():\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = params\n",
    "    df_grouped = prepare_hazard_var(log_wages, real_m_pre, real_m_post, P_o, P_b, P_s)\n",
    "    df_grouped['Scenario'] = scenario\n",
    "    df_appended = pd.concat([df_appended, df_grouped])\n",
    "    print(f\"Scenario: {scenario}\")\n",
    "    # print #rows fweight ==0\n",
    "    # print(df_grouped[df_grouped['fweight']==0].shape[0])\n",
    "    df_grouped.to_stata(f'data/df_grouped_{scenario}.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot y = fweight and remain ; x = wagcat for each scenario of df_appended. \n",
    "df_appended['remain'] = df_appended['remain'].astype(float)\n",
    "df_appended['fweight'] = df_appended['fweight'].astype(float)\n",
    "\n",
    "# Create a figure and axes for subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharey=True)\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each scenario in a separate subplot\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenarios[scenario]\n",
    "    df = df_appended[df_appended['Scenario']==scenario]\n",
    "    pre_minimum_wage_bin = df[(df['min'] == 1) & (df['period'] == \"pre\")]['wagcat'].values[0]\n",
    "    post_minimum_wage_bin = df[(df['min'] == 1) & (df['period'] == \"post\")]['wagcat'].values[0]\n",
    "\n",
    "    # Plot the data\n",
    "    axes[i].bar(df[df['period']=='pre']['wagcat'], df[df['period']=='pre']['fweight'], capsize=5, label=scenario, color = 'green', alpha = 0.5)\n",
    "    axes[i].bar(df[df['period']=='post']['wagcat'], df[df['period']=='post']['fweight'], capsize=5, label=scenario, color = 'blue', alpha = 0.5)\n",
    "    # add vline for minimum wage to bar\n",
    "    axes[i].axvline(x=pre_minimum_wage_bin, color='green', linestyle='--', linewidth=2)\n",
    "    axes[i].axvline(x=post_minimum_wage_bin, color='blue', linestyle='--', linewidth=2)\n",
    "    axes[i].legend(['pre', 'post'], loc='upper right')\n",
    "\n",
    "    axes[i].set_xlabel('Wage bin')\n",
    "    axes[i].set_title(f'{scenario}: minimum wage from {real_m_pre} to {real_m_post}, $P_o$={P_o}, $P_b$={P_b}, $P_s$={P_s}')\n",
    "    # set ylim\n",
    "    axes[i].set_ylim([0, 10_000])\n",
    "\n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'counts', va='center', rotation='vertical')\n",
    "# Set the main title\n",
    "plt.suptitle('Employment counts in each bin (fweight)', fontsize=16)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call STATA and run do.file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stata_setup\n",
    "import stata_setup\n",
    "stata_setup.config(\"/Applications/Stata/\", \"se\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%stata\n",
    "# execute the following do.file\n",
    "do \"codes/hazard simulation.do\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- As shown in both the regression results above and figures below, we observed significant positive coefficient in the bins above the minimum wage, which is confusing since there shouldn't be any spillover effect in S1 to S3. \n",
    "\n",
    "- I'm thinking if it has anything to do with the \"binomial\" assumption. Does the distribution assumption impose `fweight` or `fweight`/`remain` to follow a bell shape, or it means something else? If my guess is correct, since the distribution of S1 and S2 are truncated bell shape, to match the bell shape of binomial we got negative coefficients in bins below minimum wage and positive coefficients for bins above, even for `min1620a`. \n",
    "\n",
    "- Does the issue only happen in my simulation context? How does the full model avoid this issue with real world data (many period and states.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x-axis ranges for the variables\n",
    "# x_ranges = {\n",
    "#     'min': (0, .1),\n",
    "#     'min6b': (-2.0, -.5),\n",
    "#     'min35b': (-.5, -.2),\n",
    "#     'min12b': (-.2, .0),\n",
    "#     'min12a': (.1, .3),\n",
    "#     'min35a': (.3, .6),\n",
    "#     'min610a': (.6, 1.1),\n",
    "#     'min1115a': (1.1, 1.6),\n",
    "#     'min1620a': (1.6, 2.0)\n",
    "# }\n",
    "\n",
    "x_ranges = {\n",
    "    'min': (0, .1),\n",
    "    'min610b': (-1.0, -.5),\n",
    "    'min1115b': (-1.5, -1.0),\n",
    "    'min1620b': (-2.0, -1.5),\n",
    "    'min21b': (-2.5, -2.0),\n",
    "    'min35b': (-.5, -.2),\n",
    "    'min12b': (-.2, .0),\n",
    "    'min12a': (.1, .3),\n",
    "    'min35a': (.3, .6),\n",
    "    'min610a': (.6, 1.1),\n",
    "    'min1115a': (1.1, 1.6),\n",
    "    'min1620a': (1.6, 2.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "file_path = 'tables/glm_coefficients.xlsx'\n",
    "\n",
    "# Function to format the data\n",
    "def format_data(file_path, scenario):\n",
    "    df = pd.read_excel(file_path, sheet_name=scenario, header=None)\n",
    "    variables = df.iloc[1, 1:].values\n",
    "    coefficients = df.iloc[2, 1:].values\n",
    "    se = df.iloc[3, 1:].values\n",
    "    data = pd.DataFrame({\n",
    "        'Variable': variables,\n",
    "        'Coefficient': coefficients,\n",
    "        'SE': se,\n",
    "        'Scenario': scenario\n",
    "    })\n",
    "\n",
    "    # only keep columns where variable start with \"min\"\n",
    "    data = data[data['Variable'].str.startswith('min')==True]\n",
    "\n",
    "    return data\n",
    "\n",
    "glm_result = pd.concat([format_data(file_path, scenario) for scenario in scenarios.keys() ])\n",
    "\n",
    "# Filter the data for the variables of interest\n",
    "glm_result = glm_result[glm_result['Variable'].isin(x_ranges.keys())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare df to overlay the glm coefficients\n",
    "def prepare_fweight_df(df_appended, scenario):\n",
    "    df_appended['rel_bin'] = df_appended['wagcat'] - df_appended['min_bin']\n",
    "    df = df_appended[(df_appended['rel_bin'] >= -10) & (df_appended['rel_bin'] <= 20)]\n",
    "    # rel_dollar is the relative bin * 10\n",
    "    df['rel_dollar'] = df['rel_bin'] * 0.1\n",
    "    df_pre = df[(df['period']=='pre') & (df['Scenario']==scenario)]\n",
    "    df_post = df[(df['period']=='post') & (df['Scenario']==scenario)]\n",
    "    df_pre = df_pre[['rel_dollar', 'fweight']]\n",
    "    df_post = df_post[['rel_dollar', 'fweight']]\n",
    "    df_pre.columns = ['rel_dollar', 'fweight_pre']\n",
    "    df_post.columns = ['rel_dollar', 'fweight_post']\n",
    "    df_fweight = pd.merge(df_pre, df_post, on='rel_dollar', how='outer')\n",
    "\n",
    "    return df_fweight\n",
    "\n",
    "# prepare_fweight_df(df_appended, 'S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "file_path = 'tables/ols_coefficients.xlsx'\n",
    "\n",
    "# Function to format the data\n",
    "def format_data(file_path, scenario):\n",
    "    df = pd.read_excel(file_path, sheet_name=scenario, header=None)\n",
    "    variables = df.iloc[1, 1:].values\n",
    "    coefficients = df.iloc[2, 1:].values\n",
    "    se = df.iloc[3, 1:].values\n",
    "    data = pd.DataFrame({\n",
    "        'Variable': variables,\n",
    "        'Coefficient': coefficients,\n",
    "        'SE': se,\n",
    "        'Scenario': scenario\n",
    "    })\n",
    "\n",
    "    # only keep columns where variable start with \"min\"\n",
    "    data = data[data['Variable'].str.startswith('min')==True]\n",
    "\n",
    "    return data\n",
    "\n",
    "ols_result = pd.concat([format_data(file_path, scenario) for scenario in scenarios.keys() ])\n",
    "\n",
    "# Filter the data for the variables of interest\n",
    "ols_result = ols_result[ols_result['Variable'].isin(x_ranges.keys())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried OLS and got the same coefficients. The SE from OLS are unadjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_result['Model'] = 'glm'\n",
    "ols_result['Model'] = 'ols'\n",
    "result_appended = pd.concat([glm_result, ols_result])\n",
    "\n",
    "# Define the figure and axes for subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharey=True)\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each scenario in a separate subplot\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    real_m_pre, real_m_post, P_o, P_b, P_s = scenarios[scenario]\n",
    "    scenario_data_ols = result_appended[(result_appended['Scenario'] == scenario) & (result_appended['Model'] == 'ols')]\n",
    "    scenario_data_glm = result_appended[(result_appended['Scenario'] == scenario) & (result_appended['Model'] == 'glm')]\n",
    "    \n",
    "    x_scenario_values_ols = [np.mean([x_ranges[var][0], x_ranges[var][1]]) for var in scenario_data_ols['Variable']]  # Convert to percentage\n",
    "    y_values_ols = scenario_data_ols['Coefficient'] * 100  # Convert to percentage\n",
    "    y_err_ols = scenario_data_ols['SE'] * 100  # Convert to percentage\n",
    "    scenario_widths_ols = [(x_ranges[var][1] - x_ranges[var][0]) for var in scenario_data_ols['Variable']]  # Convert to percentage\n",
    "\n",
    "    x_scenario_values_glm = [np.mean([x_ranges[var][0], x_ranges[var][1]]) for var in scenario_data_glm['Variable']]  # Convert to percentage\n",
    "    y_values_glm = scenario_data_glm['Coefficient'] * 100  # Convert to percentage\n",
    "    y_err_glm = scenario_data_glm['SE'] * 100  # Convert to percentage\n",
    "    scenario_widths_glm = [(x_ranges[var][1] - x_ranges[var][0]) for var in scenario_data_glm['Variable']]  # Convert to percentage\n",
    "    \n",
    "    # Plot the OLS data\n",
    "    axes[i].bar(x_scenario_values_ols, y_values_ols, width=scenario_widths_ols, yerr=y_err_ols, capsize=5, label='OLS', color='green', alpha=0.5)\n",
    "    \n",
    "    # Plot the GLM data with hollow bars \n",
    "    axes[i].bar(x_scenario_values_glm, y_values_glm, width=scenario_widths_glm, yerr=y_err_glm, capsize=5, label='GLM', color='white', alpha=0.5, edgecolor='black', linewidth=1.5, hatch='//')\n",
    "    \n",
    "    # add old and new minimum wage lines with legend\n",
    "    # axes[i].axvline(x=real_m_pre - real_m_post, color='blue', linestyle='--', linewidth=2, label='Original minimum wage')\n",
    "    # axes[i].axvline(x=0, color='red', linestyle='--', linewidth=2, label='New minimum wage')\n",
    "         \n",
    "    axes[i].set_xlabel('Dollars above the minimum wage')\n",
    "    axes[i].set_title(f'{scenario}: minimum wage from {real_m_pre} to {real_m_post}, $P_o$={P_o}, $P_b$={P_b}, $P_s$={P_s}')\n",
    "    \n",
    "    # Set x-ticks with a break of 5 bins (0.5 dollars)\n",
    "    axes[i].set_xticks(np.arange(-2.5, 2.1, .5))\n",
    "    axes[i].set_xticklabels(np.arange(-2.5, 2.1, 0.5))\n",
    "\n",
    "# Set common ylabel\n",
    "fig.text(0.04, 0.5, 'Effect of the min. wage on wage bin probabilities (%)', va='center', rotation='vertical')\n",
    "\n",
    "# Set the main title\n",
    "plt.suptitle('Effect of the Minimum Wage on the Wage Distribution (OLS and GLM)', fontsize=16)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 0.96])\n",
    "\n",
    "# Add a legend\n",
    "# handles, labels = axes[0].get_legend_handles_labels()\n",
    "axes[1].legend()\n",
    "\n",
    "# fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- The hazard estimation and Cengiz et al.(2019) both shows the negative employment effect below the minimum wage, and null effect when the model has no bunching or spillover. However, the Cengiz et al.(2019) method largely depends on the numbers of employments in a given bin prior to the minimum wage change. In bins with more counts we will observe a larger negative impact (such as the original minimum wage bin). Such an effect was not observed in the hazard estimation.\n",
    "- In the hazard estimation, using the GLM and OLS gives the exactly same coefficients. The hazard estimation captures the \"drop\" for all cells below new minimum wage.\n",
    "- The naive approach that compares the wage percentile of employed workers captures fallacious spillover effects when there are not (See S1, S2, and S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
